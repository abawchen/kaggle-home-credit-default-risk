{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from colorama import Fore, Style\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from utils import ModelWrapper, XGBWrapper\n",
    "from utils.preprocess import load_all\n",
    "from utils.utils import (\n",
    "    highlight_print, timer, submit, calculate_feature_importance, load_feats,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_feats(df):\n",
    "    df['BAO_CREDIT__ANNUNITY'] = df['ACTIVE_AMT_CREDIT_SUM_SUM'] / df['AMT_ANNUITY']\n",
    "    df['BAO_CREDIT__INC'] = df['ACTIVE_AMT_CREDIT_SUM_SUM'] / df['AMT_INCOME_TOTAL']\n",
    "    df['BAO_CREDIT__CREDIT'] = df['ACTIVE_AMT_CREDIT_SUM_SUM'] / df['AMT_CREDIT']\n",
    "    df['BAO_CREDIT__GOODS'] = df['ACTIVE_AMT_CREDIT_SUM_SUM'] / df['AMT_GOODS_PRICE']\n",
    "    \n",
    "    df['PAO_CREDIT__ANNUNITY'] = df['PREV_AMT_CREDIT_MEAN'] / df['AMT_ANNUITY']\n",
    "    df['PAO_CREDIT__INC'] = df['PREV_AMT_CREDIT_MEAN'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAO_CREDIT__CREDIT'] = df['PREV_AMT_CREDIT_MEAN'] / df['AMT_CREDIT']\n",
    "    df['PAO_CREDIT__GOODS'] = df['PREV_AMT_CREDIT_MEAN'] / df['AMT_GOODS_PRICE']\n",
    "    \n",
    "    df['PAO_ANNUNITY'] = df['PREV_AMT_ANNUITY_MEAN'] / df['AMT_ANNUITY']\n",
    "    df['PAO_ANNUNITY__INC'] = df['PREV_AMT_ANNUITY_MEAN'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAO_ANNUNITY__CREDIT'] = df['PREV_AMT_ANNUITY_MEAN'] / df['AMT_CREDIT']\n",
    "    \n",
    "    df[\"IBO_PAYMENT__ACREDIT\"] = df[\"INSTAL_AMT_PAYMENT_SUM\"] / df['ACTIVE_AMT_CREDIT_SUM_SUM']\n",
    "    df[\"IBO_PAYMENT__CCREDIT\"] = df[\"INSTAL_AMT_PAYMENT_SUM\"] / df['CLOSED_AMT_CREDIT_SUM_SUM']\n",
    "    df[\"IAO_PAYMENT__ANNUITY\"] = df[\"INSTAL_AMT_PAYMENT_SUM\"] / df['AMT_ANNUITY']\n",
    "    df[\"IAO_PAYMENT__INC\"] = df[\"INSTAL_AMT_PAYMENT_SUM\"] / df['AMT_INCOME_TOTAL']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape: (356251, 951)\n",
      "\u001b[92m[Done] Load features at 2018-08-28 23:15:38.681809\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with timer('Load features'):\n",
    "    df = load_all('../data/preprocess')\n",
    "    print(\"df shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'AA_INC__ORG'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/ml-playground/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3063\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'AA_INC__ORG'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d86da3915069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mdid\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfollowing\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAA_DOC_IND_KURT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAA_LIVE_IND_SUM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAA_INC__ORG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NEW_APP_INC_BY_ORG'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AA_INC__ORG'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A_DOC_IND_KURT'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AA_DOC_IND_KURT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A_LIVE_IND_SUM'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AA_LIVE_IND_SUM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml-playground/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml-playground/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2690\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml-playground/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml-playground/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml-playground/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3063\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'AA_INC__ORG'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "expected NEW_APP_INC_BY_ORG, A_DOC_IND_KURT, A_LIVE_IND_SUM in input data\n",
    "training data did not have the following fields: AA_DOC_IND_KURT, AA_LIVE_IND_SUM, AA_INC__ORG\n",
    "\"\"\"\n",
    "# df['NEW_APP_INC_BY_ORG'] = df['AA_INC__ORG']\n",
    "# df['A_DOC_IND_KURT'] = df['AA_DOC_IND_KURT']\n",
    "# df['A_LIVE_IND_SUM'] = df['AA_LIVE_IND_SUM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_exp shape: (356251, 966)\n"
     ]
    }
   ],
   "source": [
    "df_exp = df.copy()\n",
    "df_exp = exp_feats(df)\n",
    "print(\"df_exp shape:\", df_exp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_exp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load feats from: 91_m_lgbm5_best\n",
      "feats num: 709, drop_feats: 317\n"
     ]
    }
   ],
   "source": [
    "# base = 'xgb_10x_3'\n",
    "base = '91_m_lgbm5_best'\n",
    "model_folder = os.path.join('..', 'expmodel', base)\n",
    "feats = load_feats(model_folder, 'feats.json')\n",
    "\n",
    "drop_feats = []\n",
    "if feats is not None:\n",
    "    print('Load feats from: {}'.format(base))\n",
    "    drop_feats = list(set(df_full.columns) - set(feats))\n",
    "else:\n",
    "    skip_feats = ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index'] + drop_feats\n",
    "    feats = [f for f in df_full.columns if f not in skip_feats]\n",
    "print('feats num: {}, drop_feats: {}'.format(len(feats), len(drop_feats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ylchen/.virtualenvs/ml-playground/lib/python3.6/site-packages/ipykernel_launcher.py:34: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/ylchen/.virtualenvs/ml-playground/lib/python3.6/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_full shape: (356251, 1026), selected_generated_feats: 60\n",
      "df_gen shape: (356251, 1026), feats num: 709\n"
     ]
    }
   ],
   "source": [
    "df_full, selected_generated_feats = process_generated_feats(df_full, feats)\n",
    "df_full[feats]\n",
    "print('df_full shape: {}, selected_generated_feats: {}'.format(\n",
    "    df_full.shape, len(selected_generated_feats)))\n",
    "df_gen = df_full.copy()\n",
    "print('df_gen shape: {}, feats num: {}'.format(df_gen.shape, len(feats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df shape: (307507, 1026)\n",
      "test_df shape: (48744, 1026)\n",
      "X_train df shape: (307507, 709)\n"
     ]
    }
   ],
   "source": [
    "train_df = df_gen[df_gen['TARGET'].notnull()]\n",
    "test_df = df_gen[df_gen['TARGET'].isnull()]\n",
    "print(\"train_df shape:\", train_df.shape)\n",
    "print(\"test_df shape:\", test_df.shape)\n",
    "\n",
    "X_train = train_df[feats]\n",
    "X_test = test_df[feats]\n",
    "y_train = train_df['TARGET']\n",
    "print(\"X_train df shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(n_fold, clf, feats):\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = feats\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "    fold_importance_df[\"fold\"] = n_fold+1\n",
    "    return fold_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(ModelWrapper)\n",
    "\n",
    "r = 1\n",
    "nums_fold = 5\n",
    "random_state = 1001\n",
    "prefix = '91_{}_xgb{}_{}'.format('m', nums_fold, str(r).zfill(2))\n",
    "# prefix = 'xgb_10x_3'\n",
    "model_folder = os.path.join('..', 'expmodel', '{}'.format(prefix))\n",
    "params = {}\n",
    "params = {\n",
    "    'class_weight': 'balanced',\n",
    "    'colsample_bytree': 0.8455,\n",
    "    'learning_rate': 0.0186, \n",
    "    'num_leaves': 33.5195, \n",
    "    'subsample': 0.9189, \n",
    "    'max_depth': 7.8910, \n",
    "    # 'max_depth': 8, \n",
    "    'reg_alpha': 0.0472, \n",
    "    'reg_lambda': 0.0716, \n",
    "    'min_split_gain': 0.0198,\n",
    "    'min_child_weight': 39.5617\n",
    "\n",
    "}\n",
    "params['n_estimators'] = 10000\n",
    "params['max_depth'] = int(params['max_depth'])\n",
    "params['num_leaves'] = int(params['num_leaves'])\n",
    "params['n_jobs'] = -1\n",
    "params['verbose'] = True\n",
    "\n",
    "fit_params = {}\n",
    "fit_params = {\n",
    "    'eval_metric': 'auc',\n",
    "    'early_stopping_rounds': 200,\n",
    "    'verbose': 1000,\n",
    "}\n",
    "model = XGBWrapper.XGBWrapper(\n",
    "    CLF=XGBClassifier,\n",
    "    name=\"XGBoost\",\n",
    "    model_folder=model_folder,\n",
    "    feats=feats,\n",
    "    drop_feats=drop_feats,\n",
    "    params=params,\n",
    "    fit_params=fit_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats num: 709\n",
      "model folder: ../expmodel/91_m_xgb5_01\n",
      "../expmodel/91_m_xgb5_01/XGBoost_0.pickle not exists, going to train.\n",
      "[0]\tvalidation_0-auc:0.734979\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[1000]\tvalidation_0-auc:0.794779\n",
      "Stopping. Best iteration:\n",
      "[1530]\tvalidation_0-auc:0.795736\n",
      "\n",
      "\u001b[94m- 0.795629 (2018-08-29 12:03:52.193263)\u001b[0m\n",
      "../expmodel/91_m_xgb5_01/XGBoost_1.pickle not exists, going to train.\n",
      "[0]\tvalidation_0-auc:0.729884\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[1000]\tvalidation_0-auc:0.790346\n",
      "[2000]\tvalidation_0-auc:0.792426\n",
      "Stopping. Best iteration:\n",
      "[2160]\tvalidation_0-auc:0.792582\n",
      "\n",
      "\u001b[94m- 0.792489 (2018-08-30 04:30:12.545377)\u001b[0m\n",
      "../expmodel/91_m_xgb5_01/XGBoost_2.pickle not exists, going to train.\n",
      "[0]\tvalidation_0-auc:0.731757\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n"
     ]
    }
   ],
   "source": [
    "n_fold = 0\n",
    "random_state = 1001\n",
    "feature_importance_df = pd.DataFrame()\n",
    "folds = KFold(n_splits=nums_fold, shuffle=True, random_state=random_state)\n",
    "for clf, fold_auc in model.folds_train(folds, X_train, y_train, X_test):\n",
    "    highlight_print(Fore.LIGHTBLUE_EX, '- %.6f (%s)' % (fold_auc, datetime.datetime.now()))\n",
    "    fold_importance_df = feature_importance(n_fold, clf, feats)\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    n_fold += 1\n",
    "    del clf , fold_importance_df\n",
    "    gc.collect()\n",
    "        \n",
    "score = roc_auc_score(y_train, model.oof_preds_df)\n",
    "highlight_print(Fore.RED, '## XGBoost: %.6f' % score)\n",
    "model.scores.append(score)\n",
    "model.serialize_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission/lgbm_10xx_5-2018_08_02_13_20_17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abaw/.virtualenvs/ml-playground/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "n_iter = 5\n",
    "n_splits = 5\n",
    "prefix = '106_{}_xgb{}_{}_{}'.format('b', n_splits, n_iter, str(r).zfill(2))\n",
    "model_folder = os.path.join('..', 'expmodel', prefix)\n",
    "\n",
    "b_round = 0\n",
    "b_feature_importance_df = pd.DataFrame()\n",
    "\n",
    "def xgb_evaluate(**params):\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "    global b_feature_importance_df\n",
    "    global b_round\n",
    "\n",
    "    params['n_estimators'] = 10000\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['verbose'] = True\n",
    "\n",
    "    submodel_folder = os.path.join(model_folder, str(b_round))\n",
    "    try:\n",
    "        os.makedirs(submodel_folder)\n",
    "    except:\n",
    "        pass\n",
    "    folds = KFold(n_splits=n_splits, shuffle=True, random_state=1001)\n",
    "    b_model = ModelWrapper.ModelWrapper(\n",
    "        CLF=XGBClassifier,\n",
    "        name=\"XGBBoost\",\n",
    "        model_folder=submodel_folder,\n",
    "        feats=feats,\n",
    "        drop_feats=drop_feats,\n",
    "        params=params,\n",
    "        fit_params=fit_params\n",
    "    )\n",
    "    n_fold = 0\n",
    "    for clf, fold_auc in b_model.folds_train(folds, X_train, y_train, X_test):\n",
    "        fold_importance_df = feature_importance(n_fold, clf, feats)\n",
    "        b_feature_importance_df = pd.concat([b_feature_importance_df, fold_importance_df], axis=0)\n",
    "        n_fold += 1\n",
    "        del clf, fold_importance_df\n",
    "        gc.collect()\n",
    "\n",
    "    score = roc_auc_score(y_train, b_model.oof_preds_df)\n",
    "    b_model.scores.append(score)\n",
    "    b_model.serialize_scores()\n",
    "    b_round += 1\n",
    "    return score\n",
    "\n",
    "with timer(\"BayesianOptimization:\"):\n",
    "    b_params = {'colsample_bytree': (0.8, 1),\n",
    "              'learning_rate': (.01, .02), \n",
    "              'num_leaves': (33, 35), \n",
    "              'subsample': (0.8, 1), \n",
    "              'max_depth': (7, 9), \n",
    "              'reg_alpha': (.03, .05), \n",
    "              'reg_lambda': (.06, .08), \n",
    "              'min_split_gain': (.01, .03),\n",
    "              'min_child_weight': (38, 40)}\n",
    "    bo = BayesianOptimization(xgb_evaluate, b_params)\n",
    "    bo.maximize(init_points=5, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_generated_feats(df, feats):\n",
    "    sg = {\n",
    "        '+': [], '-': [], '*': [], '/': [], '^':[]\n",
    "    }\n",
    "    x_dup_combos = [\n",
    "        ('AAAO_SOURCES_MEAN__EMPLOYED_BIRTH', 'AAO_EMPLOYED__BIRTH'),\n",
    "    ]\n",
    "    o_dup_combos = [\n",
    "        ('AAAX_CREDIT_ANNUITY__EMPLOYED', 'DAYS_EMPLOYED'),\n",
    "        ('AAO_EMPLOYED__BIRTH', 'DAYS_EMPLOYED'),\n",
    "        ('AAO_ANNUITY__INC', 'AAO_CREDIT__INC'),\n",
    "    ]\n",
    "    for feat in feats:\n",
    "        if feat.startswith('*:') or \\\n",
    "            feat.startswith('/:') or \\\n",
    "            feat.startswith('+:') or \\\n",
    "            feat.startswith('-:') or \\\n",
    "            feat.startswith('^:'):\n",
    "            operator = feat[0]\n",
    "            f1, f2 = feat[2:].split('-')\n",
    "            if operator == '*' and (f1, f2) in x_dup_combos:\n",
    "                continue\n",
    "            if operator == '/' and (f1, f2) in o_dup_combos:\n",
    "                continue\n",
    "            sg[operator].append((f1, f2))\n",
    "    operators = {\n",
    "         '*': np.multiply,\n",
    "         '/': np.divide,\n",
    "         '+': np.add,\n",
    "         '-': np.subtract,\n",
    "         '^': np.power\n",
    "    }\n",
    "    generated_feats = []\n",
    "    for k, v in sg.items():\n",
    "        for (f1, f2) in v:\n",
    "            name = '{}:{}-{}'.format(k, f1, f2)\n",
    "            generated_feats.append(name)\n",
    "            df[name] = operators[k](df[f1], df[f2])\n",
    "    return df, generated_feats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
