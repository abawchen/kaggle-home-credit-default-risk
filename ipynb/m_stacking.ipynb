{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from colorama import Fore, Style\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from utils import ModelWrapper, XGBWrapper\n",
    "from utils.utils import (\n",
    "    highlight_print, timer, submit, calculate_feature_importance, load_feats,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    _train = pd.read_csv('../data/raw/application_train.csv')\n",
    "    _test = pd.read_csv('../data/raw/application_test.csv')\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(_train), len(_test)))\n",
    "    df = _train.append(_test, sort=True).reset_index()\n",
    "\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_base_learners(pred_base_learners, x_shape, preds_key, val, y=None, n_fold=-1):\n",
    "    P = np.zeros((x_shape[0], len(pred_base_learners)))\n",
    "    for i, model in enumerate(pred_base_learners):\n",
    "        if preds_key == 'oof_preds':\n",
    "            preds_filename = os.path.join(model['model_folder'], 'oof_preds_{}.npy'.format(n_fold))\n",
    "        else:\n",
    "            preds_filename = os.path.join(model['model_folder'], 'test_preds_{}.npy'.format(n_fold))\n",
    "        p = np.load(preds_filename)\n",
    "        P[:, i] = p\n",
    "        if preds_key == 'oof_preds':\n",
    "            model[preds_key][val] = p\n",
    "        else:\n",
    "            model[preds_key] += p/val\n",
    "\n",
    "#         if y is not None:\n",
    "#             highlight_print(Fore.LIGHTBLUE_EX, '%s: %.6f' % (model['name'], roc_auc_score(y, p)))\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(base_learners, meta_learner, X):\n",
    "    \"\"\"Generate predictions from the ensemble.\"\"\"\n",
    "    P_pred = predict_base_learners(base_learners, X.shape)\n",
    "    return P_pred, meta_learner.predict_proba(P_pred)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.dataquest.io/blog/introduction-to-ensembles/\n",
    "from sklearn.base import clone\n",
    "def stacking_cv(base_learners, folds, X_train, y_train, X_test):\n",
    "\n",
    "    for model in base_learners:\n",
    "        model['oof_preds'] = np.zeros(X_train.shape[0])\n",
    "        model['test_preds'] = np.zeros(X_test.shape[0])\n",
    "\n",
    "    X_cv, y_cv = [], []\n",
    "    highlight_print(Fore.LIGHTBLUE_EX, \"Blending {} models:\".format(len(base_learners)))\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train)):\n",
    "        fold_x_train, fold_y_train = X_train.iloc[train_idx], y_train.iloc[train_idx]\n",
    "        fold_x_valid, fold_y_valid = X_train.iloc[valid_idx], y_train.iloc[valid_idx]\n",
    "\n",
    "        # Predict for fold auc score and meta_learner's input\n",
    "        fold_P_base = predict_base_learners(\n",
    "            base_learners, fold_x_valid.shape, 'oof_preds', valid_idx, fold_y_valid, n_fold=n_fold\n",
    "        )\n",
    "        X_cv.append(fold_P_base)\n",
    "        y_cv.append(fold_y_valid)\n",
    "\n",
    "        # Predict for final result\n",
    "        predict_base_learners(base_learners, X_test.shape, 'test_preds', folds.n_splits, n_fold=n_fold)\n",
    "\n",
    "    for model in base_learners:\n",
    "        score = roc_auc_score(y_train, model['oof_preds'])\n",
    "        highlight_print(Fore.RED, '- %s: %.6f' % (model['name'], score))\n",
    "    \n",
    "    X_cv = np.vstack(X_cv)\n",
    "    y_cv = np.hstack(y_cv)\n",
    "    base_test_preds = np.concatenate(\n",
    "        tuple(model['test_preds'].reshape(-1, 1) for model in base_learners), axis=1)\n",
    "\n",
    "    return X_cv, y_cv, base_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 307511, test samples: 48744\n",
      "train_df shape: (307507, 123)\n",
      "test_df shape: (48744, 123)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = preprocess()\n",
    "print(\"train_df shape:\", train_df.shape)\n",
    "print(\"test_df shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mBlending 5 models:\u001b[0m\n",
      "\u001b[31m- LightGBM: 0.793697\u001b[0m\n",
      "\u001b[31m- XGBoost: 0.793660\u001b[0m\n",
      "\u001b[31m- CatBoost: 0.791038\u001b[0m\n",
      "\u001b[31m- RandomForest: 0.758536\u001b[0m\n",
      "\u001b[31m- LogisticRegression: 0.768613\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    {\n",
    "        'name': 'LightGBM',\n",
    "        'model_folder': '../expmodel/91_m_lgbm5_best',\n",
    "    },\n",
    "    {\n",
    "        'name': 'XGBoost',\n",
    "        'model_folder': '../expmodel/xgb_10x_3',\n",
    "    },\n",
    "    {\n",
    "        'name': 'CatBoost',\n",
    "        'model_folder': '../expmodel/91_m_cat5_01',\n",
    "    },\n",
    "    {\n",
    "        'name': 'RandomForest',\n",
    "        'model_folder': '../expmodel/91_m_rf5_02',\n",
    "    },\n",
    "    {\n",
    "        'name': 'LogisticRegression',\n",
    "        'model_folder': '../expmodel/91_m_lr5_best',\n",
    "    },\n",
    "#     {\n",
    "#         'name': 'NN',\n",
    "#     }\n",
    "]\n",
    "\n",
    "num_folds = 5\n",
    "folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "X_cv, y_cv, base_test_preds = stacking_cv(models, folds, train_df, train_df['TARGET'], test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X_cv, index=range(X_cv.shape[0]))\n",
    "y_df = pd.Series(y_cv)\n",
    "base_test_df = pd.DataFrame(base_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m- #0: 0.796664\u001b[0m\n",
      "\u001b[94m- #1: 0.795836\u001b[0m\n",
      "\u001b[94m- #2: 0.795861\u001b[0m\n",
      "\u001b[94m- #3: 0.790509\u001b[0m\n",
      "\u001b[94m- #4: 0.797654\u001b[0m\n",
      "\u001b[31m## Blender: 0.794760\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "z_oof_preds = np.zeros(X_df.shape[0])\n",
    "z_test_preds = np.zeros(base_test_df.shape[0])\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_df)):\n",
    "    fold_x_train, fold_y_train = X_df.iloc[train_idx], y_df.iloc[train_idx]\n",
    "    fold_x_valid, fold_y_valid = X_df.iloc[valid_idx], y_df.iloc[valid_idx]\n",
    "    clf = XGBClassifier(**params)\n",
    "    clf.fit(\n",
    "        fold_x_train, fold_y_train.ravel(),\n",
    "        eval_set=[(fold_x_valid, fold_y_valid)],\n",
    "        eval_metric='auc', \n",
    "        verbose=False,\n",
    "        early_stopping_rounds=200\n",
    "    )\n",
    "    # validation\n",
    "    p = clf.predict_proba(fold_x_valid, ntree_limit=clf.best_ntree_limit)[:, 1]\n",
    "    z_oof_preds[valid_idx] = clf.predict_proba(fold_x_valid)[:, 1]\n",
    "    highlight_print(\n",
    "        Fore.LIGHTBLUE_EX,\n",
    "        '- #%s: %.6f' % (n_fold, roc_auc_score(fold_y_valid, p))\n",
    "    )\n",
    "    # prediction\n",
    "    z_test_preds += clf.predict_proba(base_test_df)[:, 1]/folds.n_splits\n",
    "    del clf\n",
    "    gc.collect()\n",
    "\n",
    "highlight_print(\n",
    "    Fore.RED, '## Blender: %.6f' % (roc_auc_score(y_cv, z_oof_preds))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_weight |   min_split_gain |   num_leaves |   reg_alpha |   reg_lambda |   subsample | \n"
     ]
    }
   ],
   "source": [
    "r = 1\n",
    "b_round = 0\n",
    "n_splits = 3\n",
    "prefix = '106_b_xgb_blender{}_r'.format(n_splits, str(r).zfill(2))\n",
    "folder = os.path.join('..', 'expmodel', '{}'.format(prefix))\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "        \n",
    "def xgb_evaluate(**params):\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    global b_feature_importance_df\n",
    "    global b_round\n",
    "    \n",
    "    params['n_estimators'] = 10000\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_jobs'] = -1\n",
    "    clf = XGBClassifier(**params)\n",
    "    folds = KFold(n_splits=3, shuffle=True, random_state=1001)\n",
    "    test_pred_proba = np.zeros(train_df.shape[0])\n",
    "    oof_preds = np.zeros(X_cv.shape[0])\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_df)):\n",
    "        fold_x_train, fold_y_train = X_df.iloc[train_idx], y_df.iloc[train_idx]\n",
    "        fold_x_valid, fold_y_valid = X_df.iloc[valid_idx], y_df.iloc[valid_idx]\n",
    "        clf = XGBClassifier(**params)\n",
    "        clf.fit(\n",
    "            fold_x_train, fold_y_train.ravel(),\n",
    "            eval_set=[(fold_x_valid, fold_y_valid)],\n",
    "            eval_metric='auc', \n",
    "            verbose=False,\n",
    "            early_stopping_rounds=200\n",
    "        )\n",
    "        # validation\n",
    "        p = clf.predict_proba(fold_x_valid, ntree_limit=clf.best_ntree_limit)[:, 1]\n",
    "        oof_preds[valid_idx] = p\n",
    "        b_round += 1\n",
    "\n",
    "        del fold_x_train, fold_y_train, fold_x_valid, fold_y_valid\n",
    "        gc.collect()\n",
    "\n",
    "    return roc_auc_score(y_cv, oof_preds)\n",
    "\n",
    "\n",
    "with timer(\"BayesianOptimization:\"):\n",
    "    b_params = {\n",
    "        'colsample_bytree': (0.8, 1),\n",
    "        'learning_rate': (.0, .15), \n",
    "        'num_leaves': (33, 35), #\n",
    "        'subsample': (0.75, 0.85), \n",
    "        'max_depth': (2, 4), \n",
    "        'reg_alpha': (.0, .05), \n",
    "        'reg_lambda': (.8, 1), \n",
    "        'min_split_gain': (.01, .03), #\n",
    "        'min_child_weight': (37.5, 38.5) #\n",
    "    }\n",
    "    bo = BayesianOptimization(xgb_evaluate, b_params)\n",
    "    bo.maximize(init_points=5, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats num: 0\n",
      "model folder: ../expmodel/m_stacking_xgb_01\n",
      "\u001b[94m- 0.795689 (2018-08-28 09:35:32.326754)\u001b[0m\n",
      "\u001b[94m- 0.793602 (2018-08-28 09:35:32.454463)\u001b[0m\n",
      "\u001b[94m- 0.793733 (2018-08-28 09:35:32.594618)\u001b[0m\n",
      "\u001b[94m- 0.788014 (2018-08-28 09:35:32.720201)\u001b[0m\n",
      "\u001b[94m- 0.797053 (2018-08-28 09:35:32.858999)\u001b[0m\n",
      "\u001b[31m## XGBoostBlender: 0.792592\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(XGBWrapper)\n",
    "importlib.reload(ModelWrapper)\n",
    "\n",
    "nums_fold = 5\n",
    "random_state = 1001\n",
    "\n",
    "params = {}\n",
    "params['n_jobs'] = -1\n",
    "params['n_estimators'] = 10000\n",
    "\n",
    "fit_params = {}\n",
    "fit_params = {\n",
    "    'eval_metric': 'auc', \n",
    "    'verbose': 1000,\n",
    "    'early_stopping_rounds': 200\n",
    "}\n",
    "\n",
    "r = 1\n",
    "model_folder = os.path.join('..', 'expmodel', 'm_stacking_xgb_{}'.format(str(r).zfill(2)))\n",
    "folds = KFold(n_splits=nums_fold, shuffle=True, random_state=random_state)\n",
    "model = XGBWrapper.XGBWrapper(\n",
    "    CLF=XGBClassifier,\n",
    "    name=\"XGBoostBlender\",\n",
    "    model_folder=model_folder,\n",
    "    feats=[],\n",
    "    drop_feats=[],\n",
    "    params=params,\n",
    "    fit_params=fit_params\n",
    ")\n",
    "\n",
    "n_fold = 0\n",
    "feature_importance_df = pd.DataFrame()\n",
    "for clf, fold_auc in model.folds_train(folds, X_df, y_df, base_test_df):\n",
    "    highlight_print(Fore.LIGHTBLUE_EX, '- %.6f (%s)' % (fold_auc, datetime.datetime.now()))\n",
    "    n_fold += 1\n",
    "    del clf\n",
    "    gc.collect()\n",
    "\n",
    "score = roc_auc_score(y_cv, model.oof_preds_df)\n",
    "highlight_print(Fore.RED, '## %s: %.6f' % (model.name, score))\n",
    "model.scores.append(score)\n",
    "model.serialize_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_weight |   min_split_gain |   num_leaves |   reg_alpha |   reg_lambda |   subsample | \n",
      "[0]\tvalidation_0-auc:0.785511\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[38]\tvalidation_0-auc:0.794875\n",
      "\n",
      "[0]\tvalidation_0-auc:0.78452\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[64]\tvalidation_0-auc:0.794045\n",
      "\n",
      "    1 | 00m53s | \u001b[35m   0.79346\u001b[0m | \u001b[32m            0.8725\u001b[0m | \u001b[32m         0.1022\u001b[0m | \u001b[32m     3.1642\u001b[0m | \u001b[32m           37.8453\u001b[0m | \u001b[32m          0.0298\u001b[0m | \u001b[32m     33.3997\u001b[0m | \u001b[32m     0.0148\u001b[0m | \u001b[32m      0.8236\u001b[0m | \u001b[32m     0.7723\u001b[0m | \n",
      "[0]\tvalidation_0-auc:0.784759\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[66]\tvalidation_0-auc:0.794935\n",
      "\n",
      "[0]\tvalidation_0-auc:0.784649\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[63]\tvalidation_0-auc:0.793941\n",
      "\n",
      "    2 | 00m55s | \u001b[35m   0.79371\u001b[0m | \u001b[32m            0.8786\u001b[0m | \u001b[32m         0.0738\u001b[0m | \u001b[32m     3.1573\u001b[0m | \u001b[32m           37.9045\u001b[0m | \u001b[32m          0.0133\u001b[0m | \u001b[32m     34.6473\u001b[0m | \u001b[32m     0.0234\u001b[0m | \u001b[32m      0.8116\u001b[0m | \u001b[32m     0.8466\u001b[0m | \n",
      "[0]\tvalidation_0-auc:0.760631\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[400]\tvalidation_0-auc:0.793493\n",
      "[800]\tvalidation_0-auc:0.794561\n",
      "[1200]\tvalidation_0-auc:0.794814\n",
      "Stopping. Best iteration:\n",
      "[1075]\tvalidation_0-auc:0.79482\n",
      "\n",
      "[0]\tvalidation_0-auc:0.764333\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[400]\tvalidation_0-auc:0.792026\n",
      "[800]\tvalidation_0-auc:0.793598\n",
      "[1200]\tvalidation_0-auc:0.79388\n",
      "[1600]\tvalidation_0-auc:0.793909\n",
      "[2000]\tvalidation_0-auc:0.793956\n",
      "[2400]\tvalidation_0-auc:0.793983\n",
      "Stopping. Best iteration:\n",
      "[2461]\tvalidation_0-auc:0.793985\n",
      "\n",
      "    3 | 05m58s | \u001b[35m   0.79411\u001b[0m | \u001b[32m            0.9980\u001b[0m | \u001b[32m         0.0042\u001b[0m | \u001b[32m     2.3280\u001b[0m | \u001b[32m           37.8008\u001b[0m | \u001b[32m          0.0122\u001b[0m | \u001b[32m     34.6457\u001b[0m | \u001b[32m     0.0364\u001b[0m | \u001b[32m      0.8793\u001b[0m | \u001b[32m     0.8490\u001b[0m | \n",
      "[0]\tvalidation_0-auc:0.757477\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[151]\tvalidation_0-auc:0.794822\n",
      "\n",
      "[0]\tvalidation_0-auc:0.764333\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[400]\tvalidation_0-auc:0.793979\n",
      "Stopping. Best iteration:\n",
      "[359]\tvalidation_0-auc:0.794006\n",
      "\n",
      "    4 | 01m21s | \u001b[35m   0.79423\u001b[0m | \u001b[32m            0.9225\u001b[0m | \u001b[32m         0.0433\u001b[0m | \u001b[32m     2.8806\u001b[0m | \u001b[32m           38.4299\u001b[0m | \u001b[32m          0.0168\u001b[0m | \u001b[32m     33.4165\u001b[0m | \u001b[32m     0.0479\u001b[0m | \u001b[32m      0.8324\u001b[0m | \u001b[32m     0.7943\u001b[0m | \n",
      "[0]\tvalidation_0-auc:0.763668\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[88]\tvalidation_0-auc:0.794758\n",
      "\n",
      "[0]\tvalidation_0-auc:0.755339\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[125]\tvalidation_0-auc:0.793974\n",
      "\n",
      "    5 | 00m50s |    0.79406 |             0.8168 |          0.0736 |      2.4719 |            37.9958 |           0.0229 |      33.4561 |      0.0034 |       0.9689 |      0.7563 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_weight |   min_split_gain |   num_leaves |   reg_alpha |   reg_lambda |   subsample | \n",
      "[0]\tvalidation_0-auc:0.761436\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[53]\tvalidation_0-auc:0.79485\n",
      "\n",
      "[0]\tvalidation_0-auc:0.754223\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "Stopping. Best iteration:\n",
      "[74]\tvalidation_0-auc:0.793892\n",
      "\n",
      "    6 | 01m12s |    0.79357 |             0.8275 |          0.1353 |      2.0283 |            38.4756 |           0.0151 |      34.4613 |      0.0198 |       0.8785 |      0.7895 | \n",
      "[0]\tvalidation_0-auc:0.785511\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[400]\tvalidation_0-auc:0.794936\n",
      "Stopping. Best iteration:\n",
      "[338]\tvalidation_0-auc:0.794956\n",
      "\n",
      "[0]\tvalidation_0-auc:0.784649\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[400]\tvalidation_0-auc:0.794009\n",
      "Stopping. Best iteration:\n",
      "[590]\tvalidation_0-auc:0.794019\n",
      "\n",
      "    7 | 03m00s | \u001b[35m   0.79433\u001b[0m | \u001b[32m            0.9973\u001b[0m | \u001b[32m         0.0140\u001b[0m | \u001b[32m     3.9768\u001b[0m | \u001b[32m           38.4455\u001b[0m | \u001b[32m          0.0173\u001b[0m | \u001b[32m     34.9105\u001b[0m | \u001b[32m     0.0280\u001b[0m | \u001b[32m      0.9960\u001b[0m | \u001b[32m     0.7622\u001b[0m | \n",
      "[0]\tvalidation_0-auc:0.784759\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[400]\tvalidation_0-auc:0.794864\n",
      "Stopping. Best iteration:\n",
      "[597]\tvalidation_0-auc:0.794942\n",
      "\n",
      "[0]\tvalidation_0-auc:0.784649\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[400]\tvalidation_0-auc:0.793924\n",
      "[800]\tvalidation_0-auc:0.794005\n",
      "Stopping. Best iteration:\n",
      "[834]\tvalidation_0-auc:0.794018\n",
      "\n",
      "    8 | 03m47s | \u001b[35m   0.79434\u001b[0m | \u001b[32m            0.9460\u001b[0m | \u001b[32m         0.0087\u001b[0m | \u001b[32m     3.7841\u001b[0m | \u001b[32m           38.3501\u001b[0m | \u001b[32m          0.0144\u001b[0m | \u001b[32m     33.0782\u001b[0m | \u001b[32m     0.0250\u001b[0m | \u001b[32m      0.9992\u001b[0m | \u001b[32m     0.8459\u001b[0m | \n",
      "[0]\tvalidation_0-auc:0.763668\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[400]\tvalidation_0-auc:0.787472\n",
      "[800]\tvalidation_0-auc:0.790811\n",
      "[1200]\tvalidation_0-auc:0.791736\n",
      "[1600]\tvalidation_0-auc:0.792305\n",
      "[2000]\tvalidation_0-auc:0.794187\n",
      "[2400]\tvalidation_0-auc:0.79432\n",
      "[2800]\tvalidation_0-auc:0.794462\n",
      "[3200]\tvalidation_0-auc:0.794492\n",
      "[3600]\tvalidation_0-auc:0.794559\n",
      "[4000]\tvalidation_0-auc:0.79461\n",
      "[4400]\tvalidation_0-auc:0.794713\n",
      "[4800]\tvalidation_0-auc:0.794754\n",
      "[5200]\tvalidation_0-auc:0.794792\n",
      "Stopping. Best iteration:\n",
      "[5232]\tvalidation_0-auc:0.794794\n",
      "\n",
      "[0]\tvalidation_0-auc:0.754223\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[400]\tvalidation_0-auc:0.786042\n",
      "[800]\tvalidation_0-auc:0.789872\n",
      "[1200]\tvalidation_0-auc:0.791022\n",
      "[1600]\tvalidation_0-auc:0.792416\n",
      "[2000]\tvalidation_0-auc:0.79277\n",
      "[2400]\tvalidation_0-auc:0.793252\n",
      "[2800]\tvalidation_0-auc:0.793546\n",
      "[3200]\tvalidation_0-auc:0.793608\n",
      "[3600]\tvalidation_0-auc:0.793653\n",
      "[4000]\tvalidation_0-auc:0.793708\n",
      "[4400]\tvalidation_0-auc:0.7938\n",
      "[4800]\tvalidation_0-auc:0.793838\n",
      "[5200]\tvalidation_0-auc:0.793864\n",
      "[5600]\tvalidation_0-auc:0.793876\n",
      "[6000]\tvalidation_0-auc:0.793882\n",
      "Stopping. Best iteration:\n",
      "[5964]\tvalidation_0-auc:0.793883\n",
      "\n",
      "    9 | 14m22s |    0.79411 |             0.8271 |          0.0009 |      2.7293 |            38.4239 |           0.0275 |      34.3647 |      0.0077 |       0.8203 |      0.7654 | \n",
      "[0]\tvalidation_0-auc:0.785418\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[400]\tvalidation_0-auc:0.794751\n",
      "[800]\tvalidation_0-auc:0.794932\n",
      "Stopping. Best iteration:\n",
      "[786]\tvalidation_0-auc:0.794937\n",
      "\n",
      "[0]\tvalidation_0-auc:0.782063\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[400]\tvalidation_0-auc:0.793865\n",
      "[800]\tvalidation_0-auc:0.794053\n",
      "[1200]\tvalidation_0-auc:0.794081\n",
      "Stopping. Best iteration:\n",
      "[1125]\tvalidation_0-auc:0.794095\n",
      "\n",
      "   10 | 04m17s | \u001b[35m   0.79438\u001b[0m | \u001b[32m            0.8022\u001b[0m | \u001b[32m         0.0068\u001b[0m | \u001b[32m     3.5398\u001b[0m | \u001b[32m           37.7047\u001b[0m | \u001b[32m          0.0187\u001b[0m | \u001b[32m     34.7529\u001b[0m | \u001b[32m     0.0480\u001b[0m | \u001b[32m      0.9811\u001b[0m | \u001b[32m     0.7762\u001b[0m | \n",
      "\u001b[92m[Done] BayesianOptimization: in 37:8 (2018-08-16 23:35:11.249451)\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_val': 0.7943772744528415,\n",
       " 'max_params': {'colsample_bytree': 0.802213237633085,\n",
       "  'learning_rate': 0.0067975582229254814,\n",
       "  'num_leaves': 34.75291320121073,\n",
       "  'subsample': 0.7761709648365778,\n",
       "  'max_depth': 3.539833520345912,\n",
       "  'reg_alpha': 0.048048882197007356,\n",
       "  'reg_lambda': 0.9811300424628693,\n",
       "  'min_split_gain': 0.018731692024530226,\n",
       "  'min_child_weight': 37.70474827137932}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo.res['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{'max_val': 0.7942660355460254,\n",
    " 'max_params': {'colsample_bytree': 0.8352291937136631,\n",
    "  'learning_rate': 0.010079811976522284,\n",
    "  'num_leaves': 34.16670197670736,\n",
    "  'subsample': 0.7893716094349154,\n",
    "  'max_depth': 2.2226346417231815,\n",
    "  'reg_alpha': 0.03551566886707049,\n",
    "  'reg_lambda': 0.8585271482835403,\n",
    "  'min_split_gain': 0.0239970903959675,\n",
    "  'min_child_weight': 37.769941807264956}}\n",
    "\n",
    "{'max_val': 0.7942386776844524,\n",
    " 'max_params': {'colsample_bytree': 0.8172942528381818,\n",
    "  'learning_rate': 0.010749105645269874,\n",
    "  'subsample': 0.8016649063877558,\n",
    "  'max_depth': 2.110984180833438,\n",
    "  'reg_alpha': 0.005151941589768794,\n",
    "  'reg_lambda': 0.8107495224202135}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.784833\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[200]\tvalidation_0-auc:0.795574\n",
      "[400]\tvalidation_0-auc:0.795889\n",
      "[600]\tvalidation_0-auc:0.796108\n",
      "[800]\tvalidation_0-auc:0.796177\n",
      "[1000]\tvalidation_0-auc:0.796182\n",
      "[1200]\tvalidation_0-auc:0.796191\n",
      "Stopping. Best iteration:\n",
      "[1120]\tvalidation_0-auc:0.796201\n",
      "\n",
      "\u001b[94mMeta learner 0: 0.796180\u001b[0m\n",
      "\u001b[92m[Done] XGBoost meta training 0 in 2:59 (2018-08-16 23:38:10.944664)\u001b[0m\n",
      "[0]\tvalidation_0-auc:0.783181\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[200]\tvalidation_0-auc:0.794581\n",
      "[400]\tvalidation_0-auc:0.794868\n",
      "[600]\tvalidation_0-auc:0.794963\n",
      "[800]\tvalidation_0-auc:0.794984\n",
      "Stopping. Best iteration:\n",
      "[694]\tvalidation_0-auc:0.794997\n",
      "\n",
      "\u001b[94mMeta learner 1: 0.794992\u001b[0m\n",
      "\u001b[92m[Done] XGBoost meta training 1 in 2:4 (2018-08-16 23:40:15.481199)\u001b[0m\n",
      "[0]\tvalidation_0-auc:0.773693\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[200]\tvalidation_0-auc:0.794289\n",
      "[400]\tvalidation_0-auc:0.794562\n",
      "[600]\tvalidation_0-auc:0.794757\n",
      "[800]\tvalidation_0-auc:0.794843\n",
      "[1000]\tvalidation_0-auc:0.79488\n",
      "Stopping. Best iteration:\n",
      "[956]\tvalidation_0-auc:0.794895\n",
      "\n",
      "\u001b[94mMeta learner 2: 0.794880\u001b[0m\n",
      "\u001b[92m[Done] XGBoost meta training 2 in 2:41 (2018-08-16 23:42:56.634194)\u001b[0m\n",
      "[0]\tvalidation_0-auc:0.773534\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[200]\tvalidation_0-auc:0.789386\n",
      "[400]\tvalidation_0-auc:0.789609\n",
      "[600]\tvalidation_0-auc:0.789631\n",
      "[800]\tvalidation_0-auc:0.789638\n",
      "Stopping. Best iteration:\n",
      "[767]\tvalidation_0-auc:0.789642\n",
      "\n",
      "\u001b[94mMeta learner 3: 0.789619\u001b[0m\n",
      "\u001b[92m[Done] XGBoost meta training 3 in 2:11 (2018-08-16 23:45:08.369068)\u001b[0m\n",
      "[0]\tvalidation_0-auc:0.774318\n",
      "Will train until validation_0-auc hasn't improved in 200 rounds.\n",
      "[200]\tvalidation_0-auc:0.795972\n",
      "[400]\tvalidation_0-auc:0.796168\n",
      "[600]\tvalidation_0-auc:0.796333\n",
      "[800]\tvalidation_0-auc:0.796378\n",
      "[1000]\tvalidation_0-auc:0.796404\n",
      "Stopping. Best iteration:\n",
      "[959]\tvalidation_0-auc:0.796416\n",
      "\n",
      "\u001b[94mMeta learner 4: 0.796391\u001b[0m\n",
      "\u001b[92m[Done] XGBoost meta training 4 in 2:43 (2018-08-16 23:47:51.521515)\u001b[0m\n",
      "\u001b[31m- Meta learner : 0.794270\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "params = bo.res['max']['max_params']\n",
    "params['n_jobs'] = -1\n",
    "params['n_estimators'] = 10000\n",
    "params['max_depth'] = int(params['max_depth'])\n",
    "clf = XGBClassifier(**params)\n",
    "\n",
    "oof_preds = np.zeros(X_cv.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_cv)):\n",
    "    with timer('XGBoost meta training {}'.format(n_fold)):\n",
    "        fold_x_train, fold_y_train = X_cv[train_idx.tolist(), :], y_cv.reshape(-1, 1)[train_idx.tolist(), :]\n",
    "        fold_x_valid, fold_y_valid = X_cv[valid_idx.tolist(), :], y_cv.reshape(-1, 1)[valid_idx.tolist(), :]\n",
    "        clf.fit(\n",
    "            fold_x_train, fold_y_train.ravel(),\n",
    "            eval_set=[(fold_x_valid, fold_y_valid.ravel())],\n",
    "            eval_metric='auc', \n",
    "            verbose=200,\n",
    "            early_stopping_rounds=200\n",
    "        )\n",
    "        # validation\n",
    "        oof_preds[valid_idx] = clf.predict_proba(fold_x_valid)[:, 1]\n",
    "        highlight_print(\n",
    "            Fore.LIGHTBLUE_EX,\n",
    "            'Meta learner %s: %.6f' % (n_fold, roc_auc_score(fold_y_valid, oof_preds[valid_idx.tolist()]))\n",
    "        )\n",
    "        # prediction\n",
    "        test_preds += clf.predict_proba(base_test_preds)[:, 1]/folds.n_splits\n",
    "\n",
    "highlight_print(\n",
    "    Fore.RED, '- Meta learner : %.6f' % (roc_auc_score(y_cv, oof_preds))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Meta learner 0: 0.796252\n",
    "Meta learner 1: 0.795085\n",
    "Meta learner 2: 0.794752\n",
    "Meta learner 3: 0.789691\n",
    "Meta learner 4: 0.796570\n",
    "- Meta learner : 0.794348\n",
    "\n",
    "Meta learner 0: 0.796164\n",
    "Meta learner 1: 0.795039\n",
    "Meta learner 2: 0.794793\n",
    "Meta learner 3: 0.789682\n",
    "Meta learner 4: 0.796596\n",
    "- Meta learner : 0.794330\n",
    "\n",
    "Meta learner 0: 0.796083\n",
    "Meta learner 1: 0.794803\n",
    "Meta learner 2: 0.794691\n",
    "Meta learner 3: 0.789316\n",
    "Meta learner 4: 0.796277\n",
    "- Meta learner : 0.794102\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers\n",
    "\n",
    "oof_preds = np.zeros(X_cv.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "prefix = 'nn_meta_101'\n",
    "name = '{}-{date:%Y_%m_%d_%H_%M_%S}'.format(prefix, date=datetime.datetime.now())\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_cv)):\n",
    "    with timer('NN meta training {}'.format(n_fold)):\n",
    "        \n",
    "        fold_x_train, fold_y_train = X_cv[train_idx.tolist(), :], y_cv.reshape(-1, 1)[train_idx.tolist(), :]\n",
    "        fold_x_valid, fold_y_valid = X_cv[valid_idx.tolist(), :], y_cv.reshape(-1, 1)[valid_idx.tolist(), :]\n",
    "\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(20, input_dim=fold_x_train.shape[1], activation='relu'))\n",
    "        model.add(Dense(20, activation='relu'))\n",
    "        model.add(Dense(15, activation='relu'))\n",
    "#         model.add(Dense(10, activation='relu'))\n",
    "#         model.add(Dense(10, activation='relu'))\n",
    "#         model.add(Dense(10, activation='relu'))\n",
    "#         model.add(Dense(10, activation='relu'))\n",
    "#         model.add(Dense(5, activation='relu'))\n",
    "        model.add(Dense(1, activation=\"sigmoid\", input_shape=(fold_x_train.shape[1],)))\n",
    "\n",
    "        model.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer='adam',\n",
    "            # optimizer=keras.optimizers.SGD(lr=0.01, nesterov=True),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        # callbacks = [EarlyStopping(monitor='val_loss', patience=300, verbose=1, mode='max')]\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=200)]\n",
    "        model.fit(\n",
    "            fold_x_train, fold_y_train,\n",
    "            validation_data=(fold_x_valid, fold_y_valid),\n",
    "            callbacks=callbacks,\n",
    "            shuffle=True,\n",
    "            batch_size=4096*2,\n",
    "            epochs=400,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        # validation\n",
    "        oof_preds[valid_idx] = model.predict(fold_x_valid)[:, 0]\n",
    "        highlight_print(\n",
    "            Fore.LIGHTBLUE_EX,\n",
    "            'NN blender %s: %.6f' % (n_fold, roc_auc_score(fold_y_valid, oof_preds[valid_idx.tolist()]))\n",
    "        )\n",
    "\n",
    "        # prediction\n",
    "        test_preds += model.predict(base_test_preds)[:, 0]/folds.n_splits\n",
    "    \n",
    "highlight_print(\n",
    "    Fore.RED, '- NN blender : %.6f' % (roc_auc_score(y_cv, oof_preds))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 20*2, 15\n",
    "- NN blender : 0.794022\n",
    "\n",
    "# 20*2, 5\n",
    "- NN blender : 0.794209\n",
    "\n",
    "# 20*3\n",
    "- NN blender : 0.794303\n",
    "\n",
    "# 20*2\n",
    "- Submit: 0.796\n",
    "- NN blender : 0.794344\n",
    "\n",
    "# 10*5, 5\n",
    "- NN blender : 0.794008\n",
    "\n",
    "# 20, 10*3, 1\n",
    "- NN blender : 0.794273\n",
    "\n",
    "# 4(20*3, 1) layers: add NN and replace with better xgb\n",
    "NN Meta learner 0: 0.796051\n",
    "NN Meta learner 1: 0.795375\n",
    "NN Meta learner 2: 0.794692\n",
    "NN Meta learner 3: 0.790037\n",
    "NN Meta learner 4: 0.796729\n",
    "- NN Meta learner : 0.794231\n",
    "\n",
    "# 5(20*4, 1) layers; add 1 more layer of 20\n",
    "NN Meta learner 0: 0.796164\n",
    "NN Meta learner 1: 0.794285\n",
    "NN Meta learner 2: 0.793681\n",
    "NN Meta learner 3: 0.789356\n",
    "NN Meta learner 4: 0.797046\n",
    "- NN Meta learner : 0.793909\n",
    "\n",
    "# 4(20*3, 1) layers; increase epochs from 100 to 400\n",
    "NN Meta learner 0: 0.795979\n",
    "NN Meta learner 1: 0.794495\n",
    "NN Meta learner 2: 0.794147\n",
    "NN Meta learner 3: 0.789693\n",
    "NN Meta learner 4: 0.796946\n",
    "- NN Meta learner : 0.794036\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(name)\n",
    "test_df['TARGET'] = test_preds\n",
    "test_df[['SK_ID_CURR', 'TARGET']].to_csv('../submission/{}.csv'.format(name), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=XGBClassifier(**params)\n",
    "oof_preds = np.zeros(X_cv.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_cv)):\n",
    "    with timer('XGBoost meta training {}'.format(n_fold)):\n",
    "        fold_x_train, fold_y_train = X_cv[train_idx.tolist(), :], y_cv.reshape(-1, 1)[train_idx.tolist(), :]\n",
    "        fold_x_valid, fold_y_valid = X_cv[valid_idx.tolist(), :], y_cv.reshape(-1, 1)[valid_idx.tolist(), :]\n",
    "        clf.fit(\n",
    "            fold_x_train, fold_y_train.ravel(),\n",
    "            eval_set=[(fold_x_train, fold_y_train.ravel()), (fold_x_valid, fold_y_valid.ravel())],\n",
    "            eval_metric='auc', \n",
    "            verbose=200,\n",
    "            early_stopping_rounds=200\n",
    "        )\n",
    "        # validation\n",
    "        p = clf.predict_proba(fold_x_valid, ntree_limit=clf.best_ntree_limit)[:, 1]\n",
    "        print(clf.best_iteration, roc_auc_score(fold_y_valid.ravel(), p))\n",
    "#         oof_preds[valid_idx] = clf.predict_proba(fold_x_valid)[:, 1]\n",
    "#         highlight_print(\n",
    "#             Fore.LIGHTBLUE_EX,\n",
    "#             'Meta learner %s: %.6f' % (n_fold, roc_auc_score(fold_y_valid, oof_preds[valid_idx.tolist()]))\n",
    "#         )\n",
    "        # prediction\n",
    "        test_preds += clf.predict_proba(base_test_preds)[:, 1]/folds.n_splits\n",
    "\n",
    "# highlight_print(\n",
    "#     Fore.RED, '- Meta learner : %.6f' % (roc_auc_score(y_cv, oof_preds))\n",
    "# )\n",
    "clf.best_iteration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
